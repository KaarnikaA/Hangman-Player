{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "mask function with permutations"
      ],
      "metadata": {
        "id": "yrDrM5ICzKOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import itertools\n",
        "\n",
        "def mask_word(word, mask_prob=0.5):\n",
        "    masked_entries = []\n",
        "\n",
        "    for num_to_mask in range(1, min(6, len(word))):  # Mask 1 to 5 letters\n",
        "        for _ in range(num_to_mask):\n",
        "            masked_word = list(word)\n",
        "            target_labels = []\n",
        "\n",
        "            # Randomly decide whether to mask a letter\n",
        "            indices_to_mask = random.sample(range(len(word)), num_to_mask)\n",
        "            for i in indices_to_mask:\n",
        "                target_labels.append(masked_word[i])\n",
        "                masked_word[i] = \"_\"\n",
        "\n",
        "            masked_entries.append((\"\".join(masked_word), target_labels))\n",
        "\n",
        "    return masked_entries\n",
        "\n",
        "def preprocess_word(word):\n",
        "    return [ord(letter) - ord('a') + 1 if letter != '_' else 0 for letter in word]  # Use 0 for underscore\n",
        "\n",
        "def generate_permutations(word):\n",
        "    underscores_indices = [i for i, char in enumerate(word) if char == '_']\n",
        "    num_permutations = random.randint(1, len(underscores_indices))\n",
        "\n",
        "    # Generate all possible combinations of underscores in the masked word\n",
        "    permutations_with_underscores = list(itertools.combinations(underscores_indices, num_permutations))\n",
        "\n",
        "    # Replace underscores with the corresponding letters in each permutation\n",
        "    result_permutations = []\n",
        "    for combination in permutations_with_underscores:\n",
        "        new_word = list(word)\n",
        "        for index in combination:\n",
        "            new_word[index] = '_'\n",
        "        result_permutations.append(\"\".join(new_word))\n",
        "\n",
        "    return result_permutations\n",
        "\n",
        "def process_words_from_file(file_path):\n",
        "    masked_entries_dict = {}\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        words = [line.strip() for line in file]\n",
        "\n",
        "    for word in words:\n",
        "        masked_entries = mask_word(word)\n",
        "        for masked_word, target_labels in masked_entries:\n",
        "            word_id = len(masked_entries_dict)\n",
        "            masked_entries_dict[word_id] = {'original_word': word, 'masked_word': masked_word, 'target_labels': target_labels}\n",
        "\n",
        "    return masked_entries_dict\n",
        "\n",
        "# Example usage:\n",
        "# file_path =  \"/content/words_250000_train.txt\"  # Replace with the path to your text file containing words\n",
        "# masked_entries_dict = process_words_from_file(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "g8f9lNLeLA7u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask function without permutation\n"
      ],
      "metadata": {
        "id": "h5oLVrSBzFX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "\n",
        "# def mask_word(word, mask_prob=0.5):\n",
        "#     masked_entries = []\n",
        "\n",
        "#     for num_to_mask in range(1, min(6, len(word))):  # Mask 1 to 5 letters\n",
        "#         # Randomly decide whether to mask a letter\n",
        "#         if random.random() < mask_prob:\n",
        "#             masked_word = list(word)\n",
        "#             target_labels = []\n",
        "\n",
        "#             # Randomly decide which letters to mask\n",
        "#             indices_to_mask = random.sample(range(len(word)), num_to_mask)\n",
        "#             for i in indices_to_mask:\n",
        "#                 target_labels.append(masked_word[i])\n",
        "#                 masked_word[i] = \"_\"\n",
        "\n",
        "#             masked_entries.append((\"\".join(masked_word), target_labels))\n",
        "\n",
        "#     return masked_entries\n",
        "\n",
        "# # Example usage:\n",
        "# # word = \"example\"\n",
        "# # masked_entries = mask_word(word)\n",
        "# # print(masked_entries)\n"
      ],
      "metadata": {
        "id": "OvBhCF6vwAu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "read fromm file"
      ],
      "metadata": {
        "id": "C-zw31FwzSTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Read data from a text file and store it as a list.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path (str): The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "    - data_list (list): A list containing the data read from the file.\n",
        "    \"\"\"\n",
        "    data_list = []\n",
        "\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            # Read each line from the file and append it to the list\n",
        "            for line in file:\n",
        "                data_list.append(line.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "\n",
        "    return data_list\n",
        "\n",
        "# Example usage:\n",
        "# file_path = \"/path/to/your/file.txt\"  # Replace with the actual path to your text file\n",
        "# data_list = read_data_from_file(file_path)\n",
        "\n",
        "# # Now 'data_list' contains the data read from the file\n",
        "# print(data_list)\n"
      ],
      "metadata": {
        "id": "RPk9JVR6zUhK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model - 5 layer"
      ],
      "metadata": {
        "id": "VPwOQR5HzPqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate on words\n",
        "\n"
      ],
      "metadata": {
        "id": "49LSOUcB1l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Define the function to read data from a text file\n",
        "def read_data_from_file(file_path):\n",
        "    data_list = []\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for line in file:\n",
        "                data_list.append(line.strip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "    return data_list\n",
        "\n",
        "# Function to create training data\n",
        "def create_training_data(words):\n",
        "    X, y = [], []\n",
        "    unique_chars = set(''.join(words))\n",
        "    unique_chars.add('_')  # Add underscore to the set\n",
        "    char_to_int = {char: i for i, char in enumerate(unique_chars)}\n",
        "    int_to_char = {i: char for char, i in char_to_int.items()}\n",
        "\n",
        "    for word in words:\n",
        "        for i in range(len(word)):\n",
        "            X.append(word[:i] + '_' + word[i+1:])\n",
        "            y.append(word[i])\n",
        "    return X, y, char_to_int, int_to_char\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, words, char_to_int, X_train_numeric):\n",
        "    X_eval, y_eval, _, _ = create_training_data(words)\n",
        "\n",
        "    # Convert characters to numerical values, handling characters not in char_to_int\n",
        "    X_eval_numeric = [[char_to_int[char] if char in char_to_int else 0 for char in word] for word in X_eval]\n",
        "    y_eval_numeric = [char_to_int[char] if char in char_to_int else 0 for char in y_eval]\n",
        "\n",
        "    # Pad sequences to ensure consistent length\n",
        "    X_eval_numeric = pad_sequences(X_eval_numeric, maxlen=X_train_numeric.shape[1], padding='post')\n",
        "    y_eval_numeric = np.array(y_eval_numeric)\n",
        "\n",
        "    # Reshape input for the model\n",
        "    X_eval_numeric = np.reshape(X_eval_numeric, (X_eval_numeric.shape[0], X_eval_numeric.shape[1], 1))\n",
        "\n",
        "    # Evaluate the model on masked words\n",
        "    _, accuracy = model.evaluate(X_eval_numeric, y_eval_numeric)\n",
        "    print(f\"Model Accuracy on Eval Words: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "q41G1lyS1phM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from the file\n",
        "file_path = \"/content/words_250000_train.txt\"\n",
        "words = read_data_from_file(file_path)\n",
        "\n",
        "# Create training data from the sample words\n",
        "X_train, y_train, char_to_int, int_to_char = create_training_data(words)\n",
        "\n",
        "# Modify the training data creation to use masked words\n",
        "X_train = [word[:i] + '_' + word[i + 1:] for word in words for i in range(len(word))]\n",
        "y_train = [word[i] for word in words for i in range(len(word))]\n",
        "\n",
        "# Continue with the rest of the code\n",
        "X_train_numeric = pad_sequences([[char_to_int[char] for char in entry] for entry in X_train], padding='post')\n",
        "y_train_numeric = np.array([char_to_int[char] for char in y_train])\n",
        "X_train_numeric = np.reshape(X_train_numeric, (X_train_numeric.shape[0], X_train_numeric.shape[1], 1))\n",
        "\n",
        "# Build and train the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(char_to_int), output_dim=50, input_length=X_train_numeric.shape[1]))\n",
        "model.add(LSTM(100, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(len(char_to_int), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_numeric, y_train_numeric, epochs=10, batch_size=26)\n",
        "\n",
        "# Evaluate the model on masked words\n",
        "evaluate_model(model, words, char_to_int, X_train_numeric)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gw1nLLrLcLV",
        "outputId": "a15e763e-2fe7-4bc5-8221-84d0efd9a22c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "81721/81721 [==============================] - 563s 7ms/step - loss: 1.9039 - accuracy: 0.3883\n",
            "Epoch 2/10\n",
            "81721/81721 [==============================] - 557s 7ms/step - loss: 1.5789 - accuracy: 0.5009\n",
            "Epoch 3/10\n",
            "81721/81721 [==============================] - 570s 7ms/step - loss: 1.5033 - accuracy: 0.5270\n",
            "Epoch 4/10\n",
            "81721/81721 [==============================] - 567s 7ms/step - loss: 1.4644 - accuracy: 0.5397\n",
            "Epoch 5/10\n",
            "81721/81721 [==============================] - 570s 7ms/step - loss: 1.4387 - accuracy: 0.5488\n",
            "Epoch 6/10\n",
            "81721/81721 [==============================] - 560s 7ms/step - loss: 1.4197 - accuracy: 0.5552\n",
            "Epoch 7/10\n",
            "81721/81721 [==============================] - 556s 7ms/step - loss: 1.4055 - accuracy: 0.5598\n",
            "Epoch 8/10\n",
            "81721/81721 [==============================] - 555s 7ms/step - loss: 1.3952 - accuracy: 0.5636\n",
            "Epoch 9/10\n",
            "81721/81721 [==============================] - 555s 7ms/step - loss: 1.3845 - accuracy: 0.5670\n",
            "Epoch 10/10\n",
            "81721/81721 [==============================] - 554s 7ms/step - loss: 1.3754 - accuracy: 0.5702\n",
            "66399/66399 [==============================] - 259s 4ms/step - loss: 1.2685 - accuracy: 0.5947\n",
            "Model Accuracy on Eval Words: 59.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version 2 guess"
      ],
      "metadata": {
        "id": "sUjqKeBdK3OG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def guess(model, char_to_int, current_word):\n",
        "#     # Convert the current_word to numerical values\n",
        "#     current_word_numeric = [char_to_int[char] if char in char_to_int else 0 for char in current_word]\n",
        "\n",
        "#     # Pad the sequence\n",
        "#     current_word_padded = pad_sequences([current_word_numeric], maxlen=model.input_shape[1], padding='post')\n",
        "\n",
        "#     # Reshape input for the model\n",
        "#     current_word_padded = np.reshape(current_word_padded, (current_word_padded.shape[0], current_word_padded.shape[1], 1))\n",
        "\n",
        "#     # Make a prediction using the model\n",
        "#     prediction = model.predict(current_word_padded)\n",
        "\n",
        "#     # Get the index of the predicted letter with the highest probability\n",
        "#     predicted_index = np.argmax(prediction)\n",
        "\n",
        "#     # Convert the index back to the character using int_to_char dictionary\n",
        "#     guessed_letter = int_to_char[predicted_index]\n",
        "\n",
        "#     return guessed_letter\n",
        "\n"
      ],
      "metadata": {
        "id": "fcpFs4unnoG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OlfSrFrqHt7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "from keras.models import load_model\n",
        "\n",
        "# Save the model\n",
        "model.save(\"hungman_lstm.h5\")\n",
        "model.save(\"C:/Users/Kaarvin/Downloads/hungman_lstm.h5\")\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model(\"hungman_lstm.h5\")\n",
        "\n",
        "# Example usage of guess function\n",
        "# current_word = 'boo_'  # Replace with the current masked word\n",
        "# guessed_letter = guess(loaded_model, char_to_int, current_word)\n",
        "# print(f\"Guessed Letter: {guessed_letter}\")\n",
        "\n",
        "\n",
        "# # Example usage of guess function\n",
        "# current_word = 'boo_'  # Replace with the current masked word\n",
        "# guessed_letter = guess(loaded_model, char_to_int, current_word)\n",
        "# print(f\"Guessed Letter: {guessed_letter}\")\n"
      ],
      "metadata": {
        "id": "SrUvmHxTv3BS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 params version"
      ],
      "metadata": {
        "id": "bjJgFKb3Jo-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define global variables for the model, char_to_int, and int_to_char\n",
        "global_model = None\n",
        "global_char_to_int = None\n",
        "global_int_to_char = None\n",
        "\n",
        "# Modify the guess function\n",
        "def guess(current_word, prev_guess):\n",
        "    global global_model\n",
        "    global global_char_to_int\n",
        "    global global_int_to_char\n",
        "\n",
        "    # Combine the current word and previous guess\n",
        "    input_sequence = current_word.replace('_', prev_guess)\n",
        "\n",
        "    # Convert the input sequence to numerical values\n",
        "    input_sequence_numeric = [global_char_to_int[char] if char in global_char_to_int else 0 for char in input_sequence]\n",
        "\n",
        "    # Pad the sequence\n",
        "    input_sequence_padded = pad_sequences([input_sequence_numeric], maxlen=global_model.input_shape[1], padding='post')\n",
        "\n",
        "    # Reshape input for the model\n",
        "    input_sequence_padded = np.reshape(input_sequence_padded, (input_sequence_padded.shape[0], input_sequence_padded.shape[1], 1))\n",
        "\n",
        "    # Make a prediction using the global model\n",
        "    prediction = global_model.predict(input_sequence_padded)\n",
        "\n",
        "    # Get the index of the predicted letter with the highest probability\n",
        "    predicted_index = np.argmax(prediction)\n",
        "\n",
        "    # Convert the index back to the character using global_int_to_char dictionary\n",
        "    guessed_letter = global_int_to_char[predicted_index]\n",
        "\n",
        "    return guessed_letter\n"
      ],
      "metadata": {
        "id": "7EptkPjBJrEM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set global variables\n",
        "global_model = loaded_model\n",
        "global_char_to_int = char_to_int\n",
        "global_int_to_char = int_to_char\n",
        "\n",
        "# Example usage of guess function with previous guess 'a'\n",
        "current_word = 'a__le'  # Replace with the current masked word\n",
        "prev_guess = 'p'  # Replace with the previous guess\n",
        "guessed_letter = guess(current_word, prev_guess)\n",
        "print(f\"Guessed Letter: {guessed_letter}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKWJ4YIJrls",
        "outputId": "ffa0cf83-4aaa-4f60-ebab-f966460ac1ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Guessed Letter: e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNoVmYQeJwOG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}